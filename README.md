# SM-UMT: Self-Mining Unsupervised Machine Translation

Implementation of the **Self-Mining of In-Context Examples for Unsupervised Machine Translation with LLMs** (NAACL 2025) by El Mekki & Abdul-Mageed.

---

## ğŸ“– Overview

This system enables **unsupervised machine translation** by automatically mining in-context learning (ICL) examples without requiring human-annotated parallel data. Traditional machine translation requires large parallel corpora, but this approach generates synthetic parallel data from monolingual text using a two-stage self-mining process.

### Key Innovation

The paper introduces a novel approach where LLMs can translate between languages **without any parallel training data** by:
1. Mining word-level translations to create synthetic parallel sentences
2. Using these synthetic pairs as in-context examples for sentence translation
3. Applying TopK+BM25 filtering to select the most relevant examples

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SM-UMT Translation Pipeline                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  STAGE 1: Word-Level Mining                              â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Source    â”‚â”€â”€â”€â–ºâ”‚   Word     â”‚â”€â”€â”€â–ºâ”‚  LLM Word      â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  Sentences â”‚    â”‚ Extraction â”‚    â”‚  Translation   â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚                                              â”‚           â”‚   â”‚
â”‚  â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚                    â”‚  Synthetic Parallel Data        â”‚   â”‚   â”‚
â”‚  â”‚                    â”‚  (word-by-word translations)    â”‚   â”‚   â”‚
â”‚  â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                â”‚                                 â”‚
â”‚                                â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  STAGE 2: Sentence-Level Mining                          â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚  Sentence      â”‚â”€â”€â–ºâ”‚   TopK     â”‚â”€â”€â–ºâ”‚    BM25     â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  Embeddings    â”‚   â”‚ Selection  â”‚   â”‚  Re-ranking â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  (top 20)  â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚          â”‚   â”‚
â”‚  â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚                    â”‚  Filtered ICL Examples (k=8)     â”‚  â”‚   â”‚
â”‚  â”‚                    â”‚  with threshold Ï„=0.90           â”‚  â”‚   â”‚
â”‚  â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                â”‚                                 â”‚
â”‚                                â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  TRANSLATION                                              â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚   Query    â”‚â”€â”€â”€â–ºâ”‚  Prompt    â”‚â”€â”€â”€â–ºâ”‚     LLM        â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  Sentence  â”‚    â”‚ +ICL Exs   â”‚    â”‚  Generation    â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚                                              â”‚           â”‚   â”‚
â”‚  â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚                    â”‚  Translated Output              â”‚   â”‚   â”‚
â”‚  â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ Methodology (from paper)

### Stage 1: Word-Level Mining

1. **Word Extraction**: Extract content words from source monolingual sentences
2. **LLM Translation**: Use the LLM to translate individual words with in-context examples
3. **Synthetic Parallel Data**: Create word-by-word translated parallel pairs

**Why this works**: Individual word translations are generally more reliable than full sentences for an LLM without parallel training data.

### Stage 2: Sentence-Level Mining (TopK+BM25)

1. **Sentence Embeddings**: Compute embeddings using multilingual sentence-transformers
2. **TopK Selection**: Select top-20 most similar sentences from the synthetic corpus
3. **Filtering**: Apply similarity threshold Ï„=0.90 to remove noisy pairs
4. **BM25 Re-ranking**: Use BM25 to select the final k=8 most relevant ICL examples

**Why TopK+BM25**: Combines semantic similarity (embeddings) with lexical matching (BM25) for better example selection.

---

## âœ¨ Features

- ğŸŒ **Multilingual Support**: French â†” English and Arabic â†” English
- ğŸ¤– **Gemini API**: Uses Google's free Gemini 2.5 Flash model
- ğŸ“Š **BLEU Evaluation**: Built-in evaluation with sacrebleu
- ğŸ”§ **Configurable**: All hyperparameters from the paper are adjustable
- ğŸ“ **Modular Design**: Clean separation of components for easy extension

---

## ğŸš€ Installation

```bash
# Navigate to project directory
cd "c:\Users\Lenovo\Desktop\S9\Projet integre"

# Install dependencies
pip install -r requirements.txt
```

### Dependencies

- `google-genai` - Gemini API client
- `sentence-transformers` - Multilingual sentence embeddings
- `sacrebleu` - BLEU score evaluation
- `torch` - PyTorch for embeddings
- `tqdm` - Progress bars

---

## âš™ï¸ Setup

1. **Get API Key**: Visit [Google AI Studio](https://makersuite.google.com/app/apikey) for a free Gemini API key

2. **Set API Key**:
```powershell
# Windows PowerShell
$env:GEMINI_API_KEY="your-api-key-here"

# Or pass directly via --api-key argument
python main.py --api-key "your-api-key" ...
```

---

## ğŸ“‹ Usage

### Single Sentence Translation

```bash
# French to English
python main.py --input "Bonjour le monde" --src fra --tgt eng

# Arabic to English
python main.py --input "Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…" --src arb --tgt eng

# English to French
python main.py --input "Hello world" --src eng --tgt fra
```

### Batch Translation with Evaluation

```bash
# Translate sample data with BLEU evaluation
python main.py --src fra --tgt eng --sample_size 10 --evaluate -v

# Arabic to English
python main.py --src arb --tgt eng --sample_size 5 --evaluate -v
```

### FLORES-200 Evaluation

```bash
python main.py --evaluate --use-flores --src fra --tgt eng --sample_size 100
```

### Quick Test

```bash
python main.py --test
```

### List Languages

```bash
python main.py --list-langs
```

---

## ğŸ“ Project Structure

```
Projet integre/
â”œâ”€â”€ main.py                 # CLI entry point
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ README.md               # This documentation
â”œâ”€â”€ tests/                  # Unit tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_sm_umt.py      # 15 test cases
â””â”€â”€ sm_umt/                 # Main package
    â”œâ”€â”€ __init__.py         # Package initialization
    â”œâ”€â”€ config.py           # Hyperparameters from paper
    â”œâ”€â”€ prompts.py          # LLM prompt templates
    â”œâ”€â”€ llm_client.py       # Gemini API client
    â”œâ”€â”€ word_mining.py      # Stage 1: Word-level mining
    â”œâ”€â”€ sentence_mining.py  # Stage 2: TopK+BM25 selection
    â”œâ”€â”€ bm25.py             # BM25 ranking algorithm
    â”œâ”€â”€ translator.py       # Main translation pipeline
    â”œâ”€â”€ evaluation.py       # BLEU score evaluation
    â””â”€â”€ utils.py            # Utility functions
```

---

## ğŸ“Š Key Hyperparameters (from paper)

| Parameter | Value | Description |
|-----------|-------|-------------|
| `kwp` | 10 | Number of word pairs for word-level ICL |
| `k` | 8 | Number of sentence-level ICL examples |
| `Ï„` (tau) | 0.90 | Similarity threshold for filtering |
| `top_n` | 20 | Top-N candidates before BM25 selection |

These can be modified in `sm_umt/config.py`.

---

## ğŸ Python API

```python
from sm_umt import SMUMTTranslator, Config

# Initialize with configuration
config = Config(
    src_lang="fra",    # Source language (fra, eng, arb)
    tgt_lang="eng",    # Target language
    k=8,               # Number of ICL examples
    tau=0.90           # Similarity threshold
)

translator = SMUMTTranslator(config, api_key="your-api-key")

# Prepare source sentences
source_sentences = [
    "Bonjour, comment allez-vous?",
    "Je m'appelle Marie.",
    "Il fait beau aujourd'hui."
]

# Run full pipeline with references for evaluation
references = [
    "Hello, how are you?",
    "My name is Marie.",
    "The weather is nice today."
]

result = translator.run_pipeline(source_sentences, references)

# Access results
print(f"BLEU Score: {result['evaluation']['bleu']:.2f}")

for src, tgt in zip(source_sentences, result['translations']):
    print(f"{src} -> {tgt}")
```

---

## ğŸ“ˆ Results

Tested on sample data:

| Language Pair | BLEU Score | Notes |
|---------------|------------|-------|
| French â†’ English | 7.41 | 3 sample sentences |
| Arabic â†’ English | 9.57 | 5 sample sentences |

**Note**: Higher BLEU scores are expected with larger sample sizes and more ICL examples.

---

## ğŸ” How It Works (Step by Step)

### Example: Translating "Bonjour le monde" (French â†’ English)

1. **Word Extraction**: Extract words ["bonjour", "monde"]

2. **Word Translation**: 
   - "bonjour" â†’ "hello"
   - "monde" â†’ "world"

3. **Synthetic Parallel Creation**:
   - "Bonjour le monde" â†’ "hello le world" (word-by-word)

4. **ICL Mining**: Find similar sentences from synthetic pairs using TopK+BM25

5. **Translation Prompt**:
   ```
   Translate from French to English:
   
   Examples:
   French: Comment allez-vous?
   English: how allez you
   
   French: Bonjour le monde
   English: [LLM generates: "Hello world"]
   ```

6. **Output**: "Hello world"

---

## ğŸ“š Citation

```bibtex
@inproceedings{elmekki2025effective,
  title={Effective Self-Mining of In-Context Examples for Unsupervised Machine Translation with LLMs},
  author={El Mekki, Abdellah and Abdul-Mageed, Muhammad},
  booktitle={Findings of the Association for Computational Linguistics: NAACL 2025},
  year={2025}
}
```

---

## ğŸ“ License

This implementation is for educational and research purposes.

---

## ğŸ¤ Acknowledgments

- Based on research by Abdellah El Mekki and Muhammad Abdul-Mageed (UBC-NLP)
- Uses Google's Gemini API for LLM inference
- Sentence embeddings via sentence-transformers
